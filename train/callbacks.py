import os
import time

import evaluate.recall as recall

import tensorflow as tf


class CustomProgressBar(tf.keras.callbacks.Callback):
    """Ï≤òÎ¶¨Îüâ Ï†ïÎ≥¥Î•º Ìè¨Ìï®Ìïú Ïª§Ïä§ÌÖÄ ÏßÑÌñâÎ•†Î∞î"""
    
    def __init__(self):
        super(CustomProgressBar, self).__init__()
        self.epoch_start_time = None
        self.step_start_time = None
        self.last_batch = 0
        
    def on_epoch_begin(self, epoch, logs=None):
        self.epoch_start_time = time.time()
        self.step_start_time = time.time()
        self.last_batch = 0
        
    def on_train_batch_end(self, batch, logs=None):
        if logs is None:
            return
            
        # Ï≤òÎ¶¨Îüâ Ï†ïÎ≥¥ Ï∂îÏ∂ú
        throughput = logs.get('throughput', '')
        
        # ÌòÑÏû¨ ÏãúÍ∞Ñ Í≥ÑÏÇ∞
        current_time = time.time()
        step_time = current_time - self.step_start_time
        self.step_start_time = current_time
        
        # ÏßÑÌñâÎ•†Î∞î ÏóÖÎç∞Ïù¥Ìä∏ (Í∏∞Ï°¥ Ï∂úÎ†• ÎåÄÏ≤¥)
        if batch % 10 == 0:  # 10Î∞∞ÏπòÎßàÎã§ ÏóÖÎç∞Ïù¥Ìä∏
            # Í∏∞Ï°¥ ÏßÑÌñâÎ•†Î∞î ÌòïÏãùÏóê Ï≤òÎ¶¨Îüâ Ï∂îÍ∞Ä
            accuracy = logs.get('accuracy', 0.0)
            loss = logs.get('loss', 0.0)
            
            # ÏßÑÌñâÎ•†Î∞î Ï∂úÎ†• (Í∏∞Ï°¥ ÌòïÏãù + Ï≤òÎ¶¨Îüâ)
            progress_line = f"{batch}/Unknown {step_time:.0f}s {step_time:.1f}s/step - accuracy: {accuracy:.3e} - loss: {loss:.4f}"
            if throughput:
                progress_line += f" {throughput}"
            
            print(f"\r{progress_line}", end='', flush=True)
            self.last_batch = batch
    
    def on_epoch_end(self, epoch, logs=None):
        print()  # Ï§ÑÎ∞îÍøà


class LogCallback(tf.keras.callbacks.Callback):

    def __init__(self, log_dir='./logs'):
        super(LogCallback, self).__init__()
        self.log_dir = log_dir
        self.writer = tf.summary.create_file_writer(log_dir)

    def on_train_end(self, logs=None):
        self.writer.close()

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        with self.writer.as_default():
            for name, value in logs.items():
                tf.summary.scalar(name, value, step=epoch)
            self.writer.flush()


class ThroughputCallback(tf.keras.callbacks.Callback):
    """Ï≤òÎ¶¨Îüâ(throughput) Î™®ÎãàÌÑ∞ÎßÅ ÏΩúÎ∞± - ÏßÑÌñâÎ•†Î∞î ÌÜµÌï©"""
    
    def __init__(self, total_samples, log_dir='./logs'):
        super(ThroughputCallback, self).__init__()
        self.total_samples = total_samples
        self.log_dir = log_dir
        self.writer = tf.summary.create_file_writer(log_dir)
        
        # ÏÑ±Îä• Ï∏°Ï†ï Î≥ÄÏàò
        self.epoch_start_time = None
        self.step_samples = []
        
    def on_epoch_begin(self, epoch, logs=None):
        """ÏóêÌè¨ÌÅ¨ ÏãúÏûë Ïãú ÏãúÍ∞Ñ Í∏∞Î°ù"""
        self.epoch_start_time = time.time()
        self.step_samples = []
        
    def on_train_batch_end(self, batch, logs=None):
        """Î∞∞Ïπò ÎÅùÎÇ† ÎïåÎßàÎã§ ÏÑ±Îä• Ï∏°Ï†ï"""
        if self.epoch_start_time is not None and batch > 0:
            current_time = time.time()
            
            # Î∞∞Ïπò ÌÅ¨Í∏∞ Ï∂îÏ†ï
            batch_size = logs.get('size', 32) if logs else 32
            self.step_samples.append(batch_size)
            
            # Ïã§ÏãúÍ∞Ñ Ï≤òÎ¶¨Îüâ Í≥ÑÏÇ∞
            elapsed_time = current_time - self.epoch_start_time
            total_samples_processed = sum(self.step_samples)
            samples_per_second = total_samples_processed / elapsed_time
            
            # ÏßÑÌñâÎ•†Î∞îÏóê Ï≤òÎ¶¨Îüâ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            if logs is not None:
                logs['throughput'] = f"{samples_per_second:.1f} samples/sec"
    
    def on_epoch_end(self, epoch, logs=None):
        """ÏóêÌè¨ÌÅ¨ ÎÅùÎÇ† Îïå ÏµúÏ¢Ö ÏÑ±Îä• ÌÜµÍ≥Ñ"""
        if self.epoch_start_time is not None:
            total_time = time.time() - self.epoch_start_time
            total_samples = sum(self.step_samples)
            
            # ÏÑ±Îä• ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
            samples_per_second = total_samples / total_time
            avg_time_per_sample = total_time / total_samples
            
            # GPU Ï†ïÎ≥¥
            gpu_count = len(tf.config.list_physical_devices('GPU'))
            effective_batch_size = self.step_samples[0] * gpu_count if self.step_samples else 0
            
            print(f"\nüìà Epoch {epoch + 1} Performance Summary:")
            print(f"  ‚è±Ô∏è  Total time: {total_time:.2f} seconds")
            print(f"  üìä Total samples: {total_samples:,}")
            print(f"  üöÄ Throughput: {samples_per_second:.1f} samples/second")
            print(f"  ‚ö° Avg time per sample: {avg_time_per_sample*1000:.2f} ms")
            print(f"  üéØ Effective batch size: {effective_batch_size}")
            print(f"  üñ•Ô∏è  GPUs used: {gpu_count}")
            
            # TensorBoardÏóê Í∏∞Î°ù
            with self.writer.as_default():
                tf.summary.scalar('throughput/samples_per_second', samples_per_second, step=epoch)
                tf.summary.scalar('throughput/ms_per_sample', avg_time_per_sample * 1000, step=epoch)
                tf.summary.scalar('throughput/effective_batch_size', effective_batch_size, step=epoch)
                tf.summary.scalar('throughput/gpu_count', gpu_count, step=epoch)
                self.writer.flush()
            
            # Î°úÍ∑∏Ïóê Ï∂îÍ∞Ä
            logs['throughput_samples_per_sec'] = samples_per_second
            logs['throughput_ms_per_sample'] = avg_time_per_sample * 1000
            logs['effective_batch_size'] = effective_batch_size
            logs['gpu_count'] = gpu_count

    def on_train_end(self, logs=None):
        self.writer.close()


class RecallCallback(tf.keras.callbacks.Callback):

    def __init__(self, dataset_dict, top_k, metric, log_dir='logs'):
        super(RecallCallback, self).__init__()
        self.ds_dict = dataset_dict
        self.top_k = top_k
        self.metric = metric
        self.log_dir = log_dir
        self.writer = tf.summary.create_file_writer(log_dir)

    def on_train_end(self, logs=None):
        self.writer.close()

    def on_epoch_end(self, epoch, logs=None):
        recall_avgs = {}
        # Init recall average dictionary
        for k in self.top_k:
            recall_avgs['recall@{}'.format(k)] = 0.
        # Evaluate recall over multiple datasets
        for ds_name in self.ds_dict:
            ds = self.ds_dict[ds_name]
            ds_base_name = os.path.basename(ds_name)
            recall_top_k = recall.evaluate(self.model, ds, self.metric, self.top_k, 256)
            with self.writer.as_default():
                for k, value in zip(self.top_k, recall_top_k):
                    recall_str = 'recall@{}'.format(k)
                    scalar_name = ds_base_name + '_{}'.format(recall_str)
                    value *= 100
                    tf.summary.scalar(scalar_name, value, step=epoch)
                    logs[recall_str] = tf.identity(value)
                    recall_avgs[recall_str] += value
                self.writer.flush()
        with self.writer.as_default():
            ds_size = len(self.ds_dict)
            for key in recall_avgs:
                recall_avgs[key] /= ds_size
                logs[key] = recall_avgs[key]
            self.writer.flush()

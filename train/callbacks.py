import os
import time

import evaluate.recall as recall

import tensorflow as tf


class NaNMonitorCallback(tf.keras.callbacks.Callback):
    """NaN Í∞êÏßÄ Î∞è Ï°∞Í∏∞ Ï§ëÎã® ÏΩúÎ∞±"""
    
    def __init__(self, patience=5):
        super(NaNMonitorCallback, self).__init__()
        self.patience = patience
        self.nan_count = 0
        self.last_valid_loss = None
        
    def on_train_batch_end(self, batch, logs=None):
        if logs is None:
            return
            
        current_loss = logs.get('loss', None)
        
        # NaN Í∞êÏßÄ
        if current_loss is not None and (tf.math.is_nan(current_loss) or tf.math.is_inf(current_loss)):
            self.nan_count += 1
            print(f"\n‚ö†Ô∏è  NaN/Inf detected! Batch {batch}, Loss: {current_loss}")
            print(f"   NaN count: {self.nan_count}/{self.patience}")
            
            if self.nan_count >= self.patience:
                print(f"\n‚ùå Training stopped due to NaN/Inf loss for {self.patience} consecutive batches")
                self.model.stop_training = True
        else:
            # Ïú†Ìö®Ìïú lossÍ∞Ä ÎÇòÏò§Î©¥ Ïπ¥Ïö¥ÌÑ∞ Î¶¨ÏÖã
            if current_loss is not None:
                self.nan_count = 0
                self.last_valid_loss = current_loss
                
        # LossÍ∞Ä ÎÑàÎ¨¥ ÌÅ¨Î©¥ Í≤ΩÍ≥†
        if current_loss is not None and current_loss > 100:
            print(f"\n‚ö†Ô∏è  High loss detected! Batch {batch}, Loss: {current_loss}")
            
    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            return
            
        epoch_loss = logs.get('loss', None)
        if epoch_loss is not None and (tf.math.is_nan(epoch_loss) or tf.math.is_inf(epoch_loss)):
            print(f"\n‚ùå Epoch {epoch + 1} ended with NaN/Inf loss: {epoch_loss}")
            self.model.stop_training = True


class CustomProgressBar(tf.keras.callbacks.Callback):
    """Ï≤òÎ¶¨Îüâ Ï†ïÎ≥¥Î•º Ìè¨Ìï®Ìïú Ïª§Ïä§ÌÖÄ ÏßÑÌñâÎ•†Î∞î"""
    
    def __init__(self, total_samples, batch_size):
        super(CustomProgressBar, self).__init__()
        self.epoch_start_time = None
        self.step_start_time = None
        self.last_batch = 0
        self.total_samples = total_samples
        self.batch_size = batch_size
        self.steps_per_epoch = total_samples // batch_size
        
    def on_epoch_begin(self, epoch, logs=None):
        self.epoch_start_time = time.time()
        self.step_start_time = time.time()
        self.last_batch = 0
        print(f"\nüöÄ Epoch {epoch + 1}/40 - Steps per epoch: {self.steps_per_epoch}")
        
    def on_train_batch_end(self, batch, logs=None):
        if logs is None:
            return
            
        # Ï≤òÎ¶¨Îüâ Ï†ïÎ≥¥ Ï∂îÏ∂ú
        throughput = logs.get('throughput', '')
        
        # ÌòÑÏû¨ ÏãúÍ∞Ñ Í≥ÑÏÇ∞
        current_time = time.time()
        step_time = current_time - self.step_start_time
        self.step_start_time = current_time
        
        # ÏßÑÌñâÎ•†Î∞î ÏóÖÎç∞Ïù¥Ìä∏ (10Î∞∞ÏπòÎßàÎã§ ÏóÖÎç∞Ïù¥Ìä∏)
        if batch % 10 == 0:
            # Í∏∞Ï°¥ ÏßÑÌñâÎ•†Î∞î ÌòïÏãùÏóê Ï≤òÎ¶¨Îüâ Ï∂îÍ∞Ä
            accuracy = logs.get('accuracy', 0.0)
            loss = logs.get('loss', 0.0)
            
            # ÏóêÌè¨ÌÅ¨ ÏßÑÌñâÎ•† Í≥ÑÏÇ∞
            progress = (batch / self.steps_per_epoch) * 100
            
            # ÏßÑÌñâÎ•†Î∞î Ï∂úÎ†• (ÏóêÌè¨ÌÅ¨ Í≤ΩÍ≥Ñ Ìè¨Ìï®)
            progress_line = f"{batch}/{self.steps_per_epoch} ({progress:.1f}%) {step_time:.0f}s {step_time:.1f}s/step - accuracy: {accuracy:.3e} - loss: {loss:.4f}"
            if throughput:
                progress_line += f" {throughput}"
            
            print(f"\r{progress_line}", end='', flush=True)
            self.last_batch = batch
            
            # ÏóêÌè¨ÌÅ¨ ÏôÑÎ£å Ïãú Ï§ÑÎ∞îÍøà
            if batch >= self.steps_per_epoch - 1:
                print()  # Ï§ÑÎ∞îÍøà
    
    def on_epoch_end(self, epoch, logs=None):
        print(f"\n‚úÖ Epoch {epoch + 1} completed!")
        print()


class LogCallback(tf.keras.callbacks.Callback):

    def __init__(self, log_dir='./logs'):
        super(LogCallback, self).__init__()
        self.log_dir = log_dir
        self.writer = tf.summary.create_file_writer(log_dir)

    def on_train_end(self, logs=None):
        self.writer.close()

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        with self.writer.as_default():
            for name, value in logs.items():
                tf.summary.scalar(name, value, step=epoch)
            self.writer.flush()


class ThroughputCallback(tf.keras.callbacks.Callback):
    """Ï≤òÎ¶¨Îüâ(throughput) Î™®ÎãàÌÑ∞ÎßÅ ÏΩúÎ∞± - ÏßÑÌñâÎ•†Î∞î ÌÜµÌï©"""
    
    def __init__(self, total_samples, log_dir='./logs'):
        super(ThroughputCallback, self).__init__()
        self.total_samples = total_samples
        self.log_dir = log_dir
        self.writer = tf.summary.create_file_writer(log_dir)
        
        # ÏÑ±Îä• Ï∏°Ï†ï Î≥ÄÏàò
        self.epoch_start_time = None
        self.step_samples = []
        
    def on_epoch_begin(self, epoch, logs=None):
        """ÏóêÌè¨ÌÅ¨ ÏãúÏûë Ïãú ÏãúÍ∞Ñ Í∏∞Î°ù"""
        self.epoch_start_time = time.time()
        self.step_samples = []
        
    def on_train_batch_end(self, batch, logs=None):
        """Î∞∞Ïπò ÎÅùÎÇ† ÎïåÎßàÎã§ ÏÑ±Îä• Ï∏°Ï†ï"""
        if self.epoch_start_time is not None and batch > 0:
            current_time = time.time()
            
            # Î∞∞Ïπò ÌÅ¨Í∏∞ Ï∂îÏ†ï
            batch_size = logs.get('size', 32) if logs else 32
            self.step_samples.append(batch_size)
            
            # Ïã§ÏãúÍ∞Ñ Ï≤òÎ¶¨Îüâ Í≥ÑÏÇ∞
            elapsed_time = current_time - self.epoch_start_time
            total_samples_processed = sum(self.step_samples)
            samples_per_second = total_samples_processed / elapsed_time
            
            # ÏßÑÌñâÎ•†Î∞îÏóê Ï≤òÎ¶¨Îüâ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            if logs is not None:
                logs['throughput'] = f"{samples_per_second:.1f} samples/sec"
    
    def on_epoch_end(self, epoch, logs=None):
        """ÏóêÌè¨ÌÅ¨ ÎÅùÎÇ† Îïå ÏµúÏ¢Ö ÏÑ±Îä• ÌÜµÍ≥Ñ"""
        if self.epoch_start_time is not None and self.step_samples:
            total_time = time.time() - self.epoch_start_time
            total_samples = sum(self.step_samples)
            
            # ZeroDivisionError Î∞©ÏßÄ
            if total_samples > 0 and total_time > 0:
                # ÏÑ±Îä• ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
                samples_per_second = total_samples / total_time
                avg_time_per_sample = total_time / total_samples
            
                # GPU Ï†ïÎ≥¥
                gpu_count = len(tf.config.list_physical_devices('GPU'))
                effective_batch_size = self.step_samples[0] * gpu_count if self.step_samples else 0
                
                # ÎÇ®ÏùÄ ÏóêÌè¨ÌÅ¨ ÏòàÏÉÅ ÏãúÍ∞Ñ
                remaining_epochs = 40 - (epoch + 1)  # configÏóêÏÑú Í∞ÄÏ†∏ÏôÄÏïº Ìï®
                estimated_total_time = total_time * remaining_epochs
                estimated_days = estimated_total_time / (24 * 3600)
                
                print(f"\nüìà Epoch {epoch + 1} Performance Summary:")
                print(f"  ‚è±Ô∏è  Total time: {total_time:.2f} seconds ({total_time/3600:.2f} hours)")
                print(f"  üìä Total samples: {total_samples:,}")
                print(f"  üöÄ Throughput: {samples_per_second:.1f} samples/second")
                print(f"  ‚ö° Avg time per sample: {avg_time_per_sample*1000:.2f} ms")
                print(f"  üéØ Effective batch size: {effective_batch_size}")
                print(f"  üñ•Ô∏è  GPUs used: {gpu_count}")
                print(f"  ‚è≥ Estimated remaining time: {estimated_days:.1f} days")
                
                # TensorBoardÏóê Í∏∞Î°ù
                with self.writer.as_default():
                    tf.summary.scalar('throughput/samples_per_second', samples_per_second, step=epoch)
                    tf.summary.scalar('throughput/ms_per_sample', avg_time_per_sample * 1000, step=epoch)
                    tf.summary.scalar('throughput/effective_batch_size', effective_batch_size, step=epoch)
                    tf.summary.scalar('throughput/gpu_count', gpu_count, step=epoch)
                    tf.summary.scalar('time/epoch_time_hours', total_time/3600, step=epoch)
                    tf.summary.scalar('time/estimated_remaining_days', estimated_days, step=epoch)
                    self.writer.flush()
                
                # Î°úÍ∑∏Ïóê Ï∂îÍ∞Ä
                logs['throughput_samples_per_sec'] = samples_per_second
                logs['throughput_ms_per_sample'] = avg_time_per_sample * 1000
                logs['effective_batch_size'] = effective_batch_size
                logs['gpu_count'] = gpu_count
                logs['epoch_time_hours'] = total_time/3600
                logs['estimated_remaining_days'] = estimated_days
            else:
                print(f"\n‚ö†Ô∏è  Epoch {epoch + 1}: No samples processed or zero time elapsed")

    def on_train_end(self, logs=None):
        self.writer.close()


class RecallCallback(tf.keras.callbacks.Callback):

    def __init__(self, dataset_dict, top_k, metric, log_dir='logs'):
        super(RecallCallback, self).__init__()
        self.ds_dict = dataset_dict
        self.top_k = top_k
        self.metric = metric
        self.log_dir = log_dir
        self.writer = tf.summary.create_file_writer(log_dir)

    def on_train_end(self, logs=None):
        self.writer.close()

    def on_epoch_end(self, epoch, logs=None):
        recall_avgs = {}
        # Init recall average dictionary
        for k in self.top_k:
            recall_avgs['recall@{}'.format(k)] = 0.
        # Evaluate recall over multiple datasets
        for ds_name in self.ds_dict:
            ds = self.ds_dict[ds_name]
            ds_base_name = os.path.basename(ds_name)
            recall_top_k = recall.evaluate(self.model, ds, self.metric, self.top_k, 256)
            with self.writer.as_default():
                for k, value in zip(self.top_k, recall_top_k):
                    recall_str = 'recall@{}'.format(k)
                    scalar_name = ds_base_name + '_{}'.format(recall_str)
                    value *= 100
                    tf.summary.scalar(scalar_name, value, step=epoch)
                    logs[recall_str] = tf.identity(value)
                    recall_avgs[recall_str] += value
                self.writer.flush()
        with self.writer.as_default():
            ds_size = len(self.ds_dict)
            for key in recall_avgs:
                recall_avgs[key] /= ds_size
                logs[key] = recall_avgs[key]
            self.writer.flush()

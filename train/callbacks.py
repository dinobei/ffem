import os
import time

import evaluate.recall as recall

import tensorflow as tf


class CustomProgressBar(tf.keras.callbacks.Callback):
    """ì²˜ë¦¬ëŸ‰ ì •ë³´ë¥¼ í¬í•¨í•œ ì»¤ìŠ¤í…€ ì§„í–‰ë¥ ë°”"""
    
    def __init__(self, total_samples, batch_size):
        super(CustomProgressBar, self).__init__()
        self.epoch_start_time = None
        self.step_start_time = None
        self.last_batch = 0
        self.total_samples = total_samples
        self.batch_size = batch_size
        self.steps_per_epoch = total_samples // batch_size
        
    def on_epoch_begin(self, epoch, logs=None):
        self.epoch_start_time = time.time()
        self.step_start_time = time.time()
        self.last_batch = 0
        print(f"\nğŸš€ Epoch {epoch + 1}/40 - Steps per epoch: {self.steps_per_epoch}")
        
    def on_train_batch_end(self, batch, logs=None):
        if logs is None:
            return
            
        # ì²˜ë¦¬ëŸ‰ ì •ë³´ ì¶”ì¶œ
        throughput = logs.get('throughput', '')
        
        # í˜„ì¬ ì‹œê°„ ê³„ì‚°
        current_time = time.time()
        step_time = current_time - self.step_start_time
        self.step_start_time = current_time
        
        # ì§„í–‰ë¥ ë°” ì—…ë°ì´íŠ¸ (10ë°°ì¹˜ë§ˆë‹¤ ì—…ë°ì´íŠ¸)
        if batch % 10 == 0:
            # ê¸°ì¡´ ì§„í–‰ë¥ ë°” í˜•ì‹ì— ì²˜ë¦¬ëŸ‰ ì¶”ê°€
            accuracy = logs.get('accuracy', 0.0)
            loss = logs.get('loss', 0.0)
            
            # ì—í¬í¬ ì§„í–‰ë¥  ê³„ì‚°
            progress = (batch / self.steps_per_epoch) * 100
            
            # ì§„í–‰ë¥ ë°” ì¶œë ¥ (ì—í¬í¬ ê²½ê³„ í¬í•¨)
            progress_line = f"{batch}/{self.steps_per_epoch} ({progress:.1f}%) {step_time:.0f}s {step_time:.1f}s/step - accuracy: {accuracy:.3e} - loss: {loss:.4f}"
            if throughput:
                progress_line += f" {throughput}"
            
            print(f"\r{progress_line}", end='', flush=True)
            self.last_batch = batch
            
            # ì—í¬í¬ ì™„ë£Œ ì‹œ ì¤„ë°”ê¿ˆ
            if batch >= self.steps_per_epoch - 1:
                print()  # ì¤„ë°”ê¿ˆ
    
    def on_epoch_end(self, epoch, logs=None):
        print(f"\nâœ… Epoch {epoch + 1} completed!")
        print()


class LogCallback(tf.keras.callbacks.Callback):

    def __init__(self, log_dir='./logs'):
        super(LogCallback, self).__init__()
        self.log_dir = log_dir
        self.writer = tf.summary.create_file_writer(log_dir)

    def on_train_end(self, logs=None):
        self.writer.close()

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        with self.writer.as_default():
            for name, value in logs.items():
                tf.summary.scalar(name, value, step=epoch)
            self.writer.flush()


class ThroughputCallback(tf.keras.callbacks.Callback):
    """ì²˜ë¦¬ëŸ‰(throughput) ëª¨ë‹ˆí„°ë§ ì½œë°± - ì§„í–‰ë¥ ë°” í†µí•©"""
    
    def __init__(self, total_samples, log_dir='./logs'):
        super(ThroughputCallback, self).__init__()
        self.total_samples = total_samples
        self.log_dir = log_dir
        self.writer = tf.summary.create_file_writer(log_dir)
        
        # ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜
        self.epoch_start_time = None
        self.step_samples = []
        
    def on_epoch_begin(self, epoch, logs=None):
        """ì—í¬í¬ ì‹œì‘ ì‹œ ì‹œê°„ ê¸°ë¡"""
        self.epoch_start_time = time.time()
        self.step_samples = []
        
    def on_train_batch_end(self, batch, logs=None):
        """ë°°ì¹˜ ëë‚  ë•Œë§ˆë‹¤ ì„±ëŠ¥ ì¸¡ì •"""
        if self.epoch_start_time is not None and batch > 0:
            current_time = time.time()
            
            # ë°°ì¹˜ í¬ê¸° ì¶”ì •
            batch_size = logs.get('size', 32) if logs else 32
            self.step_samples.append(batch_size)
            
            # ì‹¤ì‹œê°„ ì²˜ë¦¬ëŸ‰ ê³„ì‚°
            elapsed_time = current_time - self.epoch_start_time
            total_samples_processed = sum(self.step_samples)
            samples_per_second = total_samples_processed / elapsed_time
            
            # ì§„í–‰ë¥ ë°”ì— ì²˜ë¦¬ëŸ‰ ì •ë³´ ì¶”ê°€
            if logs is not None:
                logs['throughput'] = f"{samples_per_second:.1f} samples/sec"
    
    def on_epoch_end(self, epoch, logs=None):
        """ì—í¬í¬ ëë‚  ë•Œ ìµœì¢… ì„±ëŠ¥ í†µê³„"""
        if self.epoch_start_time is not None:
            total_time = time.time() - self.epoch_start_time
            total_samples = sum(self.step_samples)
            
            # ì„±ëŠ¥ í†µê³„ ê³„ì‚°
            samples_per_second = total_samples / total_time
            avg_time_per_sample = total_time / total_samples
            
            # GPU ì •ë³´
            gpu_count = len(tf.config.list_physical_devices('GPU'))
            effective_batch_size = self.step_samples[0] * gpu_count if self.step_samples else 0
            
            # ë‚¨ì€ ì—í¬í¬ ì˜ˆìƒ ì‹œê°„
            remaining_epochs = 40 - (epoch + 1)  # configì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
            estimated_total_time = total_time * remaining_epochs
            estimated_days = estimated_total_time / (24 * 3600)
            
            print(f"\nğŸ“ˆ Epoch {epoch + 1} Performance Summary:")
            print(f"  â±ï¸  Total time: {total_time:.2f} seconds ({total_time/3600:.2f} hours)")
            print(f"  ğŸ“Š Total samples: {total_samples:,}")
            print(f"  ğŸš€ Throughput: {samples_per_second:.1f} samples/second")
            print(f"  âš¡ Avg time per sample: {avg_time_per_sample*1000:.2f} ms")
            print(f"  ğŸ¯ Effective batch size: {effective_batch_size}")
            print(f"  ğŸ–¥ï¸  GPUs used: {gpu_count}")
            print(f"  â³ Estimated remaining time: {estimated_days:.1f} days")
            
            # TensorBoardì— ê¸°ë¡
            with self.writer.as_default():
                tf.summary.scalar('throughput/samples_per_second', samples_per_second, step=epoch)
                tf.summary.scalar('throughput/ms_per_sample', avg_time_per_sample * 1000, step=epoch)
                tf.summary.scalar('throughput/effective_batch_size', effective_batch_size, step=epoch)
                tf.summary.scalar('throughput/gpu_count', gpu_count, step=epoch)
                tf.summary.scalar('time/epoch_time_hours', total_time/3600, step=epoch)
                tf.summary.scalar('time/estimated_remaining_days', estimated_days, step=epoch)
                self.writer.flush()
            
            # ë¡œê·¸ì— ì¶”ê°€
            logs['throughput_samples_per_sec'] = samples_per_second
            logs['throughput_ms_per_sample'] = avg_time_per_sample * 1000
            logs['effective_batch_size'] = effective_batch_size
            logs['gpu_count'] = gpu_count
            logs['epoch_time_hours'] = total_time/3600
            logs['estimated_remaining_days'] = estimated_days

    def on_train_end(self, logs=None):
        self.writer.close()


class RecallCallback(tf.keras.callbacks.Callback):

    def __init__(self, dataset_dict, top_k, metric, log_dir='logs'):
        super(RecallCallback, self).__init__()
        self.ds_dict = dataset_dict
        self.top_k = top_k
        self.metric = metric
        self.log_dir = log_dir
        self.writer = tf.summary.create_file_writer(log_dir)

    def on_train_end(self, logs=None):
        self.writer.close()

    def on_epoch_end(self, epoch, logs=None):
        recall_avgs = {}
        # Init recall average dictionary
        for k in self.top_k:
            recall_avgs['recall@{}'.format(k)] = 0.
        # Evaluate recall over multiple datasets
        for ds_name in self.ds_dict:
            ds = self.ds_dict[ds_name]
            ds_base_name = os.path.basename(ds_name)
            recall_top_k = recall.evaluate(self.model, ds, self.metric, self.top_k, 256)
            with self.writer.as_default():
                for k, value in zip(self.top_k, recall_top_k):
                    recall_str = 'recall@{}'.format(k)
                    scalar_name = ds_base_name + '_{}'.format(recall_str)
                    value *= 100
                    tf.summary.scalar(scalar_name, value, step=epoch)
                    logs[recall_str] = tf.identity(value)
                    recall_avgs[recall_str] += value
                self.writer.flush()
        with self.writer.as_default():
            ds_size = len(self.ds_dict)
            for key in recall_avgs:
                recall_avgs[key] /= ds_size
                logs[key] = recall_avgs[key]
            self.writer.flush()
